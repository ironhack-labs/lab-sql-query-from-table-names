{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "00a57394-53dd-47cd-a5e7-6e7122bbb0f6",
   "metadata": {},
   "source": [
    "# SQL query from table names"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86bc2813-763c-49f2-9f5d-e1f87a20556f",
   "metadata": {
    "id": "86bc2813-763c-49f2-9f5d-e1f87a20556f"
   },
   "source": [
    "In This notebook we are going to test if using just the name of the table, and a shord definition of its contect we can use a model like GTP3.5-Turbo to select which tables are necessary to create a SQL Order to answer the user petition."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5b1be22",
   "metadata": {},
   "source": [
    "Import Statements:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7fbbdf5b-4d7f-4496-8f40-9d15ea46d023",
   "metadata": {
    "id": "7fbbdf5b-4d7f-4496-8f40-9d15ea46d023"
   },
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "import os\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "_ = load_dotenv(find_dotenv())\n",
    "\n",
    "OPENAI_API_KEY  = os.getenv('OPENAI_API_KEY')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "526632fb",
   "metadata": {},
   "source": [
    "API Key Setup:\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00d4e40e",
   "metadata": {},
   "source": [
    "Context Setup:\n",
    "- Creates a list to store conversation context\n",
    "- Adds the user's message as a system message\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9e97b9f",
   "metadata": {},
   "source": [
    "API Call:\n",
    "\n",
    "- Makes a call to OpenAI's API\n",
    "- Uses GPT-3.5-Turbo model\n",
    "- temperature=0 means responses will be more deterministic/focused\n",
    "- Returns the response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbb15abb",
   "metadata": {},
   "source": [
    "Response Handling:\n",
    "- Extracts and returns the actual response content from the API\n",
    "- response: This is the complete response object returned from the OpenAI API call\n",
    "- response.choices: The API returns an array of choices (possible responses)\n",
    "In most cases, we only get one response back, which is why we access index [0]\n",
    "- choices[0].message: Each choice contains a message object\n",
    "The message object contains the actual response from the model\n",
    "- .content: This extracts the actual text content from the message\n",
    "This is the final, clean response text we want to use"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ab89f3c",
   "metadata": {},
   "source": [
    "context =[  ] -> Creates an empty list to store the conversation context\n",
    " - Adds a message with two key components:\n",
    "  - **role: \"system\" indicates this is a system-level instruction**\n",
    "  - **content: Contains the actual user message**\n",
    "\n",
    "\n",
    "This format is required by OpenAI's chat API"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56f6a1b5",
   "metadata": {},
   "source": [
    "API Call Setup:\n",
    "- Temperature from '0 to 1' (we dont want it to be **creative** since these are SQL responses so 0 is good for now)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e480bbfb-9a80-4ea6-b792-067e63ae3148",
   "metadata": {
    "id": "e480bbfb-9a80-4ea6-b792-067e63ae3148"
   },
   "outputs": [],
   "source": [
    "#Functio to call the model.\n",
    "def return_OAI(user_message):\n",
    "    client = OpenAI(\n",
    "    # This is the default and can be omitted\n",
    "    api_key=OPENAI_API_KEY,\n",
    ")\n",
    "    context = [] #The context is like preparing a question\n",
    "\n",
    "    context.append({'role':'system', \"content\": user_message})\n",
    "    #The API call is like asking the question\n",
    "\n",
    "    response = client.chat.completions.create(    #The API call is like asking the question\n",
    "\n",
    "            model=\"gpt-3.5-turbo\", # Specifies which GPT model to use\n",
    "            messages=context, # Passes our formatted context\n",
    "            temperature=0, # Controls randomness (0 = most deterministic)\n",
    "        )    \n",
    "    return (response.choices[0].message.content)    \n",
    "\n",
    "#This RETURN line is accessing the response from GPT-3.5-Turbo in a specific way. \n",
    "#Think of it like opening a nested package:\n",
    "  #response - This is the entire response object from OpenAI's API call\n",
    "  #.choices - The API returns an array of possible responses (choices)\n",
    "  #[0] - We're accessing the first (and usually only) response in the array\n",
    "  #.message - Each choice contains a message object\n",
    "  #.content - This gets the actual text content of the message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6d5d2bdc-d822-4ed8-815e-b3f223730f15",
   "metadata": {
    "id": "6d5d2bdc-d822-4ed8-815e-b3f223730f15",
    "outputId": "61068bf0-41e3-40d9-b453-b76da5b0f086"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty DataFrame\n",
      "Columns: [table, definition]\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "#Definition of the tables.\n",
    "import pandas as pd\n",
    "\n",
    "# Table and definitions sample / # ENTER A TABLE COLUMNS HERE,  # ENTER A TABLE DEFINITATIONS HERE\n",
    "data = {\n",
    "    'table': [],  # List of table columns\n",
    "    'definition': []  # List of corresponding definitions\n",
    "}\n",
    "\n",
    "# Create DataFrame\n",
    "df = pd.DataFrame(data)\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "009e681c-d95d-4c9c-b044-5bfd76e3ad95",
   "metadata": {
    "id": "009e681c-d95d-4c9c-b044-5bfd76e3ad95"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "text_tables = '\\n'.join([f\"{row['table']}: {row['definition']}\" for index, row in df.iterrows()])\n",
    "\n",
    "#'\\n' creates a new line\n",
    "#here, it iterates through each table and definition row and evaluates each expresion and combine \n",
    "#the tables into a readable format\n",
    "\n",
    "print(text_tables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c7e275ae-f20d-4134-b9b6-d8677dfb544c",
   "metadata": {
    "id": "c7e275ae-f20d-4134-b9b6-d8677dfb544c"
   },
   "outputs": [],
   "source": [
    "prompt_question_tables = \"\"\"\n",
    "Given the following tables and their content definitions,\n",
    "###Tables\n",
    "{tables}\n",
    "\n",
    "Tell me which tables would be necessary to query with SQL to address the user's question below.\n",
    "Return the table names in a json format.\n",
    "###User Question:\n",
    "{question}\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b1cb5957-2df2-4e5e-9e6a-ace955c9817e",
   "metadata": {
    "id": "b1cb5957-2df2-4e5e-9e6a-ace955c9817e"
   },
   "outputs": [],
   "source": [
    "#Creating the prompt, with the user questions and the tables definitions.\n",
    "pqt1 = prompt_question_tables.format(tables=text_tables, question=\"Which movie won the most Oscars\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  - format() Method: This line is filling the placeholders in the prompt_question_tables template you defined earlier.\n",
    "  - The {tables} placeholder is being replaced by the content stored in text_tables (the result of the earlier code which generated table and definition pairs).\n",
    "  - The {question} placeholder is being replaced by the question \"Which movie won the most Oscars\".\n",
    "**Purpose: This dynamically generates a prompt for GPT-3.5 to help identify which tables from your database structure are necessary to answer the question.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "10d30f2b-6c23-4fd6-8038-840fba784cce",
   "metadata": {
    "id": "10d30f2b-6c23-4fd6-8038-840fba784cce",
    "outputId": "9924022c-7b2b-4ec8-e2c2-75bc1c745151",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "```json\n",
      "{\n",
      "    \"tables\": {\n",
      "        \"movies\": \"Contains information about movies\",\n",
      "        \"awards\": \"Contains information about awards won by movies\"\n",
      "    }\n",
      "}\n",
      "```\n"
     ]
    }
   ],
   "source": [
    "print(return_OAI(pqt1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "57e07083-be8f-4cd0-95bd-c4b909422c6b",
   "metadata": {
    "id": "57e07083-be8f-4cd0-95bd-c4b909422c6b"
   },
   "outputs": [],
   "source": [
    "pqt3 = prompt_question_tables.format(tables=text_tables,question=\"Which actors have won the most Oscars?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a0eeb79e-caf1-4f48-9897-168d95d2ae37",
   "metadata": {
    "id": "a0eeb79e-caf1-4f48-9897-168d95d2ae37",
    "outputId": "81d77115-9cad-4284-a228-5368bb9aa6fb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "```json\n",
      "{\n",
      "    \"tables\": [\"actors\", \"awards\"]\n",
      "}\n",
      "```\n"
     ]
    }
   ],
   "source": [
    "print(return_OAI(pqt3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "321bb9a2-4937-4e9a-a31b-7049cb8f5aa3",
   "metadata": {
    "id": "321bb9a2-4937-4e9a-a31b-7049cb8f5aa3"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "fc023776",
   "metadata": {},
   "source": [
    "# Exercise\n",
    " - Complete the prompts similar to what we did in class. \n",
    "     - Try a few versions if you have time\n",
    "     - Be creative\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8d3c9306",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Define Tables and Corresponding Descriptions\n",
    "data = {\n",
    "    'table': ['Gross Revenue', 'Movie', 'Actors','Director', 'Award', 'Year of Release'],\n",
    "    'definition': [\n",
    "        'Contains customer details such as how much money the studio earned with its premiere',\n",
    "        'Contains the name of the movie',\n",
    "        'Contains the name of the actors',\n",
    "        'Contains the name of the director',\n",
    "        'Contains the number of awards and specifies the Category of the award',\n",
    "        'Contains the year of release.'\n",
    "    ]\n",
    "}\n",
    "\n",
    "# Create DataFrame\n",
    "df = pd.DataFrame(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3879acfd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "```json\n",
      "{\n",
      "    \"tables\": [\"movies\", \"awards\"]\n",
      "}\n",
      "```\n",
      "```json\n",
      "{\n",
      "    \"tables\": [\"actors\", \"awards\"]\n",
      "}\n",
      "```\n",
      "```json\n",
      "{\n",
      "    \"tables\": [\"directors\", \"awards\"]\n",
      "}\n",
      "```\n",
      "```json\n",
      "{\n",
      "    \"tables\": [\"movies\"]\n",
      "}\n",
      "```\n"
     ]
    }
   ],
   "source": [
    "pqt1 = prompt_question_tables.format(tables=text_tables, question=\"Which movie won the most Oscars?\")\n",
    "print(return_OAI(pqt1))\n",
    "\n",
    "# Prompt 2 (you can complete this one)\n",
    "pqt2 = prompt_question_tables.format(tables=text_tables, question=\"Which actors have won the most Oscars?\")\n",
    "print(return_OAI(pqt2))\n",
    "\n",
    "# Prompt 3 (Feel free to experiment and complete this one)\n",
    "pqt3 = prompt_question_tables.format(tables=text_tables, question=\"Which directors have won the most Oscars?\")\n",
    "print(return_OAI(pqt3))\n",
    "\n",
    "# Prompt 4 (Be creative and complete this one)\n",
    "pqt4 = prompt_question_tables.format(tables=text_tables, question=\"Which movie had the most gross revenue?\") \n",
    "print(return_OAI(pqt4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3a438c8",
   "metadata": {},
   "source": [
    " - Write a one page report summarizing your findings.\n",
    "     - Were there variations that didn't work well? i.e., where GPT either hallucinated or wrong\n",
    " - What did you learn?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2149ed85",
   "metadata": {},
   "source": [
    "In this lab, we used GPT-3.5 to generate SQL queries based on table names and definitions, leveraging prompts that ask GPT-3.5 to identify which database tables are relevant to answer specific questions. The goal was to see how well GPT-3.5 could interpret table structures and provide accurate table mappings for SQL queries.\n",
    "\n",
    "Prompts Tested:\n",
    "I tested a series of prompts to evaluate how GPT-3.5 would respond in different scenarios, focusing on querying information related to movies, actors, and awards.\n",
    "\n",
    "Examples of Prompts Tested:\n",
    "\n",
    "\"Which movie won the most Oscars?\"\n",
    "Result: GPT-3.5 identified the relevant tables as \"movies\" and \"awards\".\n",
    "\"Which actors have won the most Oscars?\"\n",
    "Result: GPT-3.5 returned the tables \"actors\" and \"awards\", showing consistency in correctly mapping awards-related queries.\n",
    "\"Which directors have won the most Oscars?\"\n",
    "Result: The model correctly mapped to \"directors\" and \"awards\" tables.\n",
    "Challenges and Errors Encountered:\n",
    "Error with Table and Definition Mismatch:\n",
    "\n",
    "While building the table schema in the DataFrame, I encountered errors related to mismatches in the length of the table names and their definitions. Initially, there was a ValueError that indicated all arrays must be of the same length. After correcting this, the DataFrame was successfully created.\n",
    "Hallucinations or Inaccurate Table Mappings:\n",
    "\n",
    "\"Which movie genres have won the most Oscars?\"\n",
    "GPT-3.5 failed to provide accurate table suggestions when queried about genres. This indicated that the model was either hallucinating or the table structures were not well understood in relation to the data.\n",
    "More Complex Queries:\n",
    "When I tried more complex queries, such as combining different entities (e.g., movies and revenue), GPT-3.5 sometimes returned irrelevant or incomplete table mappings.\n",
    "What Worked Well:\n",
    "Simpler questions related directly to entities defined in the database schema worked well, and GPT-3.5 accurately provided the required tables in JSON format. Examples of this include questions related to specific actors, movies, or awards.\n",
    "The model responded accurately when the queries were direct and tied closely to predefined table names and definitions.\n",
    "What Didn’t Work Well:\n",
    "For more abstract queries, such as asking about movie genres or attempting to combine too many fields in one query, GPT-3.5 struggled to provide accurate responses.\n",
    "Additionally, when questions were vague or implied a deeper relationship between entities (like revenue and award wins), GPT-3.5 returned tables that didn’t directly match the expected SQL tables.\n",
    "What Did I Learn:\n",
    "Through this exercise, I learned the importance of structuring data and prompts carefully when working with language models. GPT-3.5 was highly effective at identifying simple and direct table mappings, but it struggled when the queries became more complex or ambiguous. The quality of the prompt directly influenced the quality of the response, and having clear table definitions helped in generating more accurate SQL queries. Furthermore, this lab demonstrated that while GPT-3.5 is powerful, it still has limitations in understanding intricate database relationships.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "include_colab_link": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "newenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
